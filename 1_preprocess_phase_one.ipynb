{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing phase 1:\n",
    "##    - Splitting hours from an original file\n",
    "##    - Save its data on seperate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# WARNING: install below packages\n",
    "# !conda install openpyxl\n",
    "# !conda install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.getcwd()\n",
    "original_dataset_path = os.path.join(root_path, 'datasets', 'Original_MCP_Data')\n",
    "preprocessed_dataset_path = os.path.join(root_path, 'datasets', 'Preprocess_Phase_1')\n",
    "\n",
    "os.makedirs(name=preprocessed_dataset_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_need_preprocessing_paths(load_path):\n",
    "    # Create an empty list to store path of files that need preprocessing\n",
    "    \n",
    "    need_preprocessing = []\n",
    "\n",
    "    for year in os.listdir(path=load_path):\n",
    "        for month in os.listdir(path=os.path.join(load_path, year)):\n",
    "            month_path = os.path.join(load_path, year, month)\n",
    "            \n",
    "            need_preprocessing += [os.path.join(month_path, day) for day in os.listdir(path=month_path)]\n",
    "\n",
    "    return need_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_time(x):\n",
    "    day = x[:2]\n",
    "    month = x[3:5]\n",
    "    year = x[6:10]\n",
    "    hour = x[11:13]\n",
    "    minute = x[14:16]\n",
    "    second = x[17:19]\n",
    "    return f'{year}-{month}-{day} {hour}:{minute}:{second}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_hour_data(one_hour_original_data, time_column, invalid_column_names, save_path):\n",
    "\n",
    "    # Create a new empty dictionary for storing one hour final data\n",
    "    one_hour_final_data = {}\n",
    "\n",
    "    # Check if the first row of the first column of the one hour original data is 'Bid curve chart data (Reference time)'\n",
    "    # In another word, check if the first column contain column names\n",
    "    assert one_hour_original_data.iloc[0, 0] == time_column\n",
    "    \n",
    "    ################################################\n",
    "    # Add name, value to empty dictionary\n",
    "    #===============================================\n",
    "    # For each row\n",
    "    for row_index in range(len(one_hour_original_data)):\n",
    "        name, value = one_hour_original_data.iloc[row_index, 0], one_hour_original_data.iloc[row_index, 1]\n",
    "\n",
    "        # Check if the name is not in the invalid column names\n",
    "        if name not in invalid_column_names:\n",
    "\n",
    "            # Creating column names\n",
    "            if name not in one_hour_final_data:\n",
    "                one_hour_final_data[name] = []\n",
    "\n",
    "            # Fixing 'Bid curve chart data (Reference time)'\n",
    "            if name == time_column:\n",
    "                value = value.replace(' +', '')\n",
    "\n",
    "                assert len(value) == 19, f'Time should have 19 characters, but got {len(value)}'\n",
    "\n",
    "                # Change time format\n",
    "                value = correct_time(x=value)  # %Y-%m-%d %H:%M:%S\n",
    "                # Determine save path\n",
    "                save_path = os.path.join(save_path, value[0:4], value[5:7], value[8:10], value[11:13])\n",
    "                # Original save name\n",
    "                original_save_name = value.replace('-', '_').replace(' ', '_').replace(':', '_')\n",
    "\n",
    "            # Add this value to the empty dictionary\n",
    "            one_hour_final_data[name].append(value)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # All name, value in one_hour_final_data\n",
    "    # should have the same length\n",
    "    #===============================================\n",
    "    # Get maximum number of values in one_hour_final_data\n",
    "    max_count_values = max([len(v) for v in one_hour_final_data.values()])\n",
    "\n",
    "    # Checking length of names and values in one_hour_final_data\n",
    "    for k, v in one_hour_final_data.items():\n",
    "\n",
    "        # Number of values in 'Price value' and 'Volume value' should be equal to max_count_values\n",
    "        if len(v) != 1:\n",
    "            assert len(v) == max_count_values, f'{k} should have {max_count_values} values, but got {len(v)}'\n",
    "\n",
    "        # Other names should have only 1 value. It will be broadcast max_count_values times\n",
    "        else:\n",
    "            one_hour_final_data[k] = [v[0] for _ in range(max_count_values)]\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    # Create one_hour_final_data dataframe\n",
    "    one_hour_final_data = pd.DataFrame(\n",
    "        data=one_hour_final_data,\n",
    "        columns=one_hour_final_data.keys()\n",
    "    )\n",
    "    one_hour_final_data[time_column] = pd.to_datetime(arg=one_hour_final_data[time_column], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Create directories for storing this new prepared data if they are not exist\n",
    "    os.makedirs(name=save_path, exist_ok=True)\n",
    "\n",
    "    ################################################\n",
    "    # Check if the save name already exists\n",
    "    # (for different data with exactly the same date)\n",
    "    #===============================================\n",
    "    # Saving file\n",
    "    if len(original_save_name) != 19:\n",
    "        raise ValueError(f'Invalid tmp name {original_save_name}!')\n",
    "    else:\n",
    "        duplicate_count = 1\n",
    "        save_name = f'{original_save_name}.csv'\n",
    "        while os.path.exists(path=os.path.join(save_path, save_name)):\n",
    "            save_name = f'{original_save_name}_{duplicate_count}.csv'\n",
    "            duplicate_count += 1\n",
    "\n",
    "    one_hour_final_data.to_csv(\n",
    "        path_or_buf=os.path.join(save_path, save_name),\n",
    "        columns=one_hour_final_data.columns,\n",
    "        index=True\n",
    "    )\n",
    "    ################################################\n",
    "    \n",
    "    # Delete this file in RAM for efficiency\n",
    "    del one_hour_final_data\n",
    "\n",
    "    # Clear the RAM\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(file_path, time_column, invalid_column_names, save_path):\n",
    "\n",
    "    # Read the original file\n",
    "    original_dataset = pd.read_excel(io=file_path, header=None)\n",
    "\n",
    "    # Number of columns should be even\n",
    "    assert len(original_dataset.columns) % 2 == 0, f'Error in {file_path} file:\\n' \\\n",
    "        f'Number of columns should be even, but got {len(original_dataset.columns)}'\n",
    "\n",
    "    # For every even number (column index) in range of number of original dataset columns\n",
    "    for i in range(0, len(original_dataset.columns), 2):\n",
    "\n",
    "        # process every two column (one hour data) and then save that in separate files\n",
    "        process_one_hour_data(\n",
    "            one_hour_original_data=original_dataset.iloc[:, i:i+2],\n",
    "            time_column=time_column,\n",
    "            invalid_column_names=invalid_column_names,\n",
    "            save_path=save_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_phase_one(data_path, save_path, invalid_column_names, time_column):\n",
    "\n",
    "    # Get file paths that need preprocessing\n",
    "    need_preprocessing_paths = get_need_preprocessing_paths(load_path=data_path)\n",
    "\n",
    "    # For displaying the process as progressive bar\n",
    "    t = tqdm(need_preprocessing_paths)\n",
    "\n",
    "    # for each file that needs preprocessing\n",
    "    for path in t:\n",
    "        \n",
    "        t.set_description_str(desc=f'Preprocessing {path}')\n",
    "\n",
    "        # Process the file\n",
    "        process_one_file(\n",
    "            file_path=path,\n",
    "            time_column=time_column,\n",
    "            invalid_column_names=invalid_column_names,\n",
    "            save_path=save_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing /home/naeim_md93/Projects/NordPool_MCP_Forecasting/datasets/Original_MCP_Data/2021/07_July_2021/mcp_data_report_23-07-2021-00_00_00.xls: 100%|██████████| 3636/3636 [4:34:39<00:00,  4.53s/it]        \n"
     ]
    }
   ],
   "source": [
    "preprocess_phase_one(\n",
    "    data_path=original_dataset_path,\n",
    "    save_path=preprocessed_dataset_path,\n",
    "    invalid_column_names=[float('nan'), None, np.nan],\n",
    "    time_column='Bid curve chart data (Reference time)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad34e93d731a436a079e6a01faba719d62fbdfb24e912969a23f180123473e58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
